{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"C:\\\\Users\\\\nitikadam\\\\Desktop\\\\Python_Notebooks\\\\Resume_Cleaning_And_Classification\\\\Data\\\\train_data\\\\Final_Data\\\\Final_CV1.csv\")\n",
    "df_train=df_train.apply(lambda x: x.astype(str).str.lower())\n",
    "#df_train=df_train.reset_index()\n",
    "df_test=pd.read_csv(\"C:\\\\Users\\\\nitikadam\\\\Desktop\\\\Python_Notebooks\\\\Resume_Cleaning_And_Classification\\\\Data\\\\train_data\\\\Final_Data\\\\Final_CV2.csv\")\n",
    "df_test=df_test.apply(lambda x: x.astype(str).str.lower())\n",
    "#df_test=df_test.reset_index()\n",
    "labels=df_test[\"Entities\"].unique()\n",
    "df_train= df_train.groupby('Statements').agg({ 'Entities': ','.join, 'Annotation': ','.join }).reset_index()\n",
    "df_test= df_test.groupby('Statements').agg({ 'Entities': ','.join, 'Annotation': ','.join }).reset_index()\n",
    "\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'profile summary: having 3.10 years of it experience and currently working big data analyst.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[8,\"Statements\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statements</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: eilps7814d 3 marital status : single 4 langu...</td>\n",
       "      <td>demographic_information_address</td>\n",
       "      <td>new delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>btech ocjp scjp fresher arpit saraf contact no...</td>\n",
       "      <td>demographic_information_address,demographic_in...</td>\n",
       "      <td>16 26 a tilak nagar new delhi -110018,91-97034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>company: unitedhealth group duration: mar-2016...</td>\n",
       "      <td>organization_information,role_information</td>\n",
       "      <td>unitedhealth group,big data developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>currently working on multiple big data project...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>big data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datastage unix oracle sql.</td>\n",
       "      <td>technologies</td>\n",
       "      <td>unix oracle sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>environment: unix tools: data stage 8.7 databa...</td>\n",
       "      <td>role_information</td>\n",
       "      <td>etl developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>experience in methodologies like agile waterfa...</td>\n",
       "      <td>project_methodology ,project_methodology</td>\n",
       "      <td>agile,waterfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gadgetwood eservices pvt ltd. oct 2013 may 201...</td>\n",
       "      <td>organization_information</td>\n",
       "      <td>gadgetwood eservices pvt ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>profile summary: having 3.10 years of it exper...</td>\n",
       "      <td>total_experience,role_information</td>\n",
       "      <td>3.10 years,big data analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>responsible for all the activities related to ...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>etl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>skill set: application technology hadoop pyspa...</td>\n",
       "      <td>certifications</td>\n",
       "      <td>ibm certified solution developer infosphere da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>work experience in big data ecosystems with va...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>work experience: united health group optum tec...</td>\n",
       "      <td>role_information</td>\n",
       "      <td>software engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statements  \\\n",
       "0   : eilps7814d 3 marital status : single 4 langu...   \n",
       "1   btech ocjp scjp fresher arpit saraf contact no...   \n",
       "2   company: unitedhealth group duration: mar-2016...   \n",
       "3   currently working on multiple big data project...   \n",
       "4                          datastage unix oracle sql.   \n",
       "5   environment: unix tools: data stage 8.7 databa...   \n",
       "6   experience in methodologies like agile waterfa...   \n",
       "7   gadgetwood eservices pvt ltd. oct 2013 may 201...   \n",
       "8   profile summary: having 3.10 years of it exper...   \n",
       "9   responsible for all the activities related to ...   \n",
       "10  skill set: application technology hadoop pyspa...   \n",
       "11  work experience in big data ecosystems with va...   \n",
       "12  work experience: united health group optum tec...   \n",
       "\n",
       "                                             Entities  \\\n",
       "0                     demographic_information_address   \n",
       "1   demographic_information_address,demographic_in...   \n",
       "2           organization_information,role_information   \n",
       "3                                        technologies   \n",
       "4                                        technologies   \n",
       "5                                    role_information   \n",
       "6           project_methodology ,project_methodology    \n",
       "7                            organization_information   \n",
       "8                   total_experience,role_information   \n",
       "9                                        technologies   \n",
       "10                                     certifications   \n",
       "11                                       technologies   \n",
       "12                                   role_information   \n",
       "\n",
       "                                           Annotation  \n",
       "0                                           new delhi  \n",
       "1   16 26 a tilak nagar new delhi -110018,91-97034...  \n",
       "2               unitedhealth group,big data developer  \n",
       "3                                            big data  \n",
       "4                                     unix oracle sql  \n",
       "5                                       etl developer  \n",
       "6                                     agile,waterfall  \n",
       "7                        gadgetwood eservices pvt ltd  \n",
       "8                         3.10 years,big data analyst  \n",
       "9                                                 etl  \n",
       "10  ibm certified solution developer infosphere da...  \n",
       "11                                             hadoop  \n",
       "12                                  software engineer  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statements</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>himanshu kumar singh sr. systems engineer info...</td>\n",
       "      <td>demographic_information_contact,demographic_in...</td>\n",
       "      <td>91-8249164460,himanshusinghcs0036@gmail.com,in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>responsibilities: writing spark scala code for...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>spark scala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skills learned: spark aws emr s3 aurora db ath...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>aws emr s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>used hadoop stack hive spark airflow scheduler...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>writing python code for database connection an...</td>\n",
       "      <td>technologies</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Statements  \\\n",
       "0  himanshu kumar singh sr. systems engineer info...   \n",
       "1  responsibilities: writing spark scala code for...   \n",
       "2  skills learned: spark aws emr s3 aurora db ath...   \n",
       "3  used hadoop stack hive spark airflow scheduler...   \n",
       "4  writing python code for database connection an...   \n",
       "\n",
       "                                            Entities  \\\n",
       "0  demographic_information_contact,demographic_in...   \n",
       "1                                       technologies   \n",
       "2                                       technologies   \n",
       "3                                       technologies   \n",
       "4                                       technologies   \n",
       "\n",
       "                                          Annotation  \n",
       "0  91-8249164460,himanshusinghcs0036@gmail.com,in...  \n",
       "1                                        spark scala  \n",
       "2                                         aws emr s3  \n",
       "3                                             hadoop  \n",
       "4                                             python  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new delhi\n",
      "16 26 a tilak nagar new delhi -110018\n",
      "91-9703496654\n",
      "arpitsarafjain2002@gmail.com\n",
      "unitedhealth group\n",
      "big data developer\n",
      "big data\n",
      "unix oracle sql\n",
      "etl developer\n",
      "agile\n",
      "waterfall\n",
      "gadgetwood eservices pvt ltd\n",
      "3.10 years\n",
      "big data analyst\n",
      "etl\n",
      "ibm certified solution developer infosphere datastage v9.1\n",
      "hadoop\n",
      "software engineer\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA=[]\n",
    "for i in range(len(df_train.index)):\n",
    "        annotations_extracted=str(df_train.loc[i,'Annotation']).split(\",\")\n",
    "        entities_extracted=str(df_train.loc[i,'Entities']).split(\",\")\n",
    "        data=[]\n",
    "        #print(i)\n",
    "        for j in range(len(annotations_extracted)):\n",
    "            annotation=annotations_extracted[j].strip()\n",
    "            print(annotation)\n",
    "            a = re.search(r'\\b(%s)\\b'%annotation, df_train.loc[i,'Statements'])\n",
    "            #print(a)\n",
    "            start_index=a.start()\n",
    "            end_index=int(start_index)+len(annotation)\n",
    "            data.append((start_index,end_index,entities_extracted[j]))\n",
    "        TRAIN_DATA.append((df_train.loc[i,'Statements'].strip(),{'entities':data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91-8249164460\n",
      "himanshusinghcs0036@gmail.com\n",
      "infosys ltd\n",
      "chandigarh\n",
      "sr. systems engineer\n",
      "spark scala\n",
      "aws emr s3\n",
      "hadoop\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA=[]\n",
    "for i in range(len(df_test.index)):\n",
    "        annotations_extracted=str(df_test.loc[i,'Annotation']).split(\",\")\n",
    "        entities_extracted=str(df_test.loc[i,'Entities']).split(\",\")\n",
    "        data=[]\n",
    "        for j in range(len(annotations_extracted)):\n",
    "            annotation=annotations_extracted[j].strip()\n",
    "            print(annotation)\n",
    "            a = re.search(r'\\b(%s)\\b'%annotation, df_test.loc[i,'Statements'])\n",
    "            #print(a)\n",
    "            start_index=a.start()\n",
    "            end_index=int(start_index)+len(annotation)\n",
    "            data.append((start_index,end_index,entities_extracted[j]))\n",
    "        TEST_DATA.append((df_test.loc[i,'Statements'].strip(),{'entities':data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(': eilps7814d 3 marital status : single 4 languages known : hindi english 5 present address : 16 26a tilak nagar new delhi-110018 place: new delhi arpit saraf 1',\n",
       "  {'entities': [(112, 121, 'demographic_information_address')]}),\n",
       " ('btech ocjp scjp fresher arpit saraf contact no : +91-9703496654 email : arpitsarafjain2002@gmail.com residential address : 16 26 a tilak nagar new delhi -110018 dob : 20 02 1992 professional objective to serve as a developer in an organization that provides me ample room for exhibiting my data warehousing and big data skills while continuously feeding my insatiable appetite to learn and implement the skills acquired.',\n",
       "  {'entities': [(123, 160, 'demographic_information_address'),\n",
       "    (50, 63, 'demographic_information_contact'),\n",
       "    (72, 100, 'demographic_information_email')]}),\n",
       " ('company: unitedhealth group duration: mar-2016- till now designation role: big data developer skills involved: hadoop pyspark hive sqoop python oozie responsibilities: working on pyspark to sink up hdfs.',\n",
       "  {'entities': [(9, 27, 'organization_information'),\n",
       "    (75, 93, 'role_information')]}),\n",
       " ('currently working on multiple big data projects in uhg and assisting their big data effort and rolling out big data solutions by transforming loading and processing large diverse datasets using nosql and hadoop ecosystem.',\n",
       "  {'entities': [(30, 38, 'technologies')]}),\n",
       " ('datastage unix oracle sql.', {'entities': [(10, 25, 'technologies')]}),\n",
       " ('environment: unix tools: data stage 8.7 database: oracle database 10g scheduler : tivoli work scheduler role: etl developer contributions: 1.',\n",
       "  {'entities': [(110, 123, 'role_information')]}),\n",
       " ('experience in methodologies like agile waterfall and scrum.',\n",
       "  {'entities': [(33, 38, 'project_methodology '),\n",
       "    (39, 48, 'project_methodology ')]}),\n",
       " ('gadgetwood eservices pvt ltd. oct 2013 may 2014 associate software developer primary skills: sql pl sql gadgetwood started with an idea of offering users in india with dependable technical support and quality repair service in consumer electronics smartphones tablets backed by a knowledgeable certified team of resources serving customers online and at the doorstep job responsibilities: create multiple queries to access the database.',\n",
       "  {'entities': [(0, 28, 'organization_information')]}),\n",
       " ('profile summary: having 3.10 years of it experience and currently working big data analyst.',\n",
       "  {'entities': [(24, 34, 'total_experience'), (74, 90, 'role_information')]}),\n",
       " ('responsible for all the activities related to requirement gathering analysing requirement development implementation and support of etl processes for large scale data.',\n",
       "  {'entities': [(132, 135, 'technologies')]}),\n",
       " ('skill set: application technology hadoop pyspark hive sqoop hbase datastage database technologies ibm db2 and oracle programming languages pl sql unix shell scripting tools cloudera oozie itg xl deploy certification recognition : ibm certified solution developer infosphere datastage v9.1 won team award company name: uhg for extensive contribution to the project.',\n",
       "  {'entities': [(230, 288, 'certifications')]}),\n",
       " ('work experience in big data ecosystems with varying level of expertise around different hadoop technologies like hive spark python sqoop hbase oozie.',\n",
       "  {'entities': [(88, 94, 'technologies')]}),\n",
       " ('work experience: united health group optum technologies may 2014-till date associate software engineer may 2014 feb 2016 software engineer feb 2016 till date primary skills: pyspark hadoop hive python sqoop hbase oozie etl unix professional projects ndb network database network database is the principal repository of both participating and non-participating provider information.',\n",
       "  {'entities': [(85, 102, 'role_information')]})]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('himanshu kumar singh sr. systems engineer infosys ltd chandigarh +91-8249164460 himanshusinghcs0036@gmail.com date of birth: 5th jan 1993 designation senior systems engineer key role production support data analyst hadoop developer etl developer project 1: prime therapeutics overview: develop migrate etl pipeline on big data which includes putting the data from fs to hive and apply data quality and data validation rule and create report on daily weekly and monthly basis.',\n",
       "  {'entities': [(66, 79, 'demographic_information_contact'),\n",
       "    (80, 109, 'demographic_information_email'),\n",
       "    (42, 53, 'organization_information'),\n",
       "    (54, 64, 'demographic_information_address'),\n",
       "    (21, 41, 'role_information')]}),\n",
       " ('responsibilities: writing spark scala code for data processing.',\n",
       "  {'entities': [(26, 37, 'technologies')]}),\n",
       " ('skills learned: spark aws emr s3 aurora db athena redshift mysql python hive unix airflow scheduler.',\n",
       "  {'entities': [(22, 32, 'technologies')]}),\n",
       " ('used hadoop stack hive spark airflow scheduler unix.',\n",
       "  {'entities': [(5, 11, 'technologies')]}),\n",
       " ('writing python code for database connection and write java rest service to connect ui and backend service.',\n",
       "  {'entities': [(8, 14, 'technologies')]})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "model=None\n",
    "n_iter= 50\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    for n_iter in np.arange(50,70,5):\n",
    "        for dropout in np.arange(0.4,0.7,0.05):\n",
    "            optimizer = nlp.begin_training()\n",
    "            for itn in range(n_iter):\n",
    "                random.shuffle(TRAIN_DATA)\n",
    "                losses = {}\n",
    "                for text, annotations in TRAIN_DATA:\n",
    "                    nlp.update(\n",
    "                        [text],  # batch of texts\n",
    "                        [annotations],  # batch of annotations\n",
    "                        drop=dropout,  # dropout - make it harder to memorise data\n",
    "                        sgd=optimizer,  # callable to update weights\n",
    "                        losses=losses)\n",
    "                #print(\"Iteration:\"+str(itn+1)+\"dropout:\"+str(dropout)+\" loss:\"+str(losses))\n",
    "            predicted_annotations=[]\n",
    "            predicted_entities=[]\n",
    "            for text, _ in TEST_DATA:\n",
    "                doc = nlp(text)\n",
    "                #print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "                #print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "                result= [(ent.text, ent.label_) for ent in doc.ents]\n",
    "                predicted_annotations.append([ent.text for ent in doc.ents])\n",
    "                predicted_entities.append([ent.label_ for ent in doc.ents])\n",
    "            df_test['predicted_annotations'] = pd.Series(predicted_annotations, index = df_test.index[:len(predicted_annotations)])\n",
    "            df_test['predicted_entities'] = pd.Series(predicted_entities, index = df_test.index[:len(predicted_entities)])\n",
    "            labels=df_test[\"Entities\"].unique()\n",
    "            cols = ['predicted_annotations', 'predicted_entities']\n",
    "            for col in cols:\n",
    "                df_test[col] = df_test[col].map(lambda x: str(x).lstrip(\"[\").rstrip(\"[\")).astype(str)\n",
    "            for col in cols:\n",
    "                df_test[col] = df_test[col].map(lambda x: str(x).lstrip(\"]\").rstrip(\"]\")).astype(str)\n",
    "            true_positives=0\n",
    "            false_positives=0\n",
    "            false_negetives=0\n",
    "            true_negetives=0\n",
    "            entities_actual=df_test[\"Entities\"].values\n",
    "            annotations_actual=df_test[\"Annotation\"].values\n",
    "            annotations_predicted=df_test['predicted_annotations'].values\n",
    "            entities_predicted=df_test['predicted_entities'].values\n",
    "            for k in range(len(labels)):\n",
    "                for i in range(len(entities_actual)):\n",
    "                    pred_ent=str(entities_predicted[i]).split(\",\")\n",
    "                    act_ent=str(entities_actual[i]).split(\",\")\n",
    "                    pred_annot=str(annotations_predicted[i]).split(\",\")\n",
    "                    act_annot=str(annotations_actual[i]).split(\",\")\n",
    "                    for j in range(len(act_ent)):\n",
    "                        for l in range(len(pred_ent)):\n",
    "                            if (labels[k] in act_ent[j]):\n",
    "                                if(labels[k] in pred_ent[l]):\n",
    "                                    if act_annot[j] in pred_annot[l]:\n",
    "                                        true_positives=true_positives+1\n",
    "                                    else:\n",
    "                                        false_negetives=false_negetives+1\n",
    "                    if (labels[k] in entities_predicted[i])&(labels[k] not in entities_actual[i]):\n",
    "                        false_positives=false_positives+1\n",
    "                    elif (labels[k] not in entities_predicted[i])&(labels[k] in entities_actual[i]):\n",
    "                        false_negetives=false_negetives+1\n",
    "                    else:\n",
    "                        true_negetives=true_negetives+1\n",
    "    \n",
    "            print(\"Dropout is:\",dropout)\n",
    "            print(\"Iterations are:\",n_iter)\n",
    "            Precision=(true_positives)/(true_positives+false_positives)\n",
    "            Recall=(true_positives)/(true_positives+false_negetives)\n",
    "            Fscore=2*((Precision*Recall)/(Precision+Recall))\n",
    "            print (\"precision:\",Precision)\n",
    "            #print(\"true\")\n",
    "            print(\"Recall:\",Recall)\n",
    "            print(\"F-Score:\",Fscore)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "iteration:1 loss:{'ner': 216.0386017426565}\n",
      "iteration:2 loss:{'ner': 61.51631315638653}\n",
      "iteration:3 loss:{'ner': 58.53014491488493}\n",
      "iteration:4 loss:{'ner': 66.70084653566917}\n",
      "iteration:5 loss:{'ner': 51.71290623408147}\n",
      "iteration:6 loss:{'ner': 65.60579387160455}\n",
      "iteration:7 loss:{'ner': 48.55805671871308}\n",
      "iteration:8 loss:{'ner': 46.854365792430414}\n",
      "iteration:9 loss:{'ner': 51.35514763903874}\n",
      "iteration:10 loss:{'ner': 50.89845918592978}\n",
      "iteration:11 loss:{'ner': 32.97755827577984}\n",
      "iteration:12 loss:{'ner': 37.19192798757634}\n",
      "iteration:13 loss:{'ner': 53.324757003648536}\n",
      "iteration:14 loss:{'ner': 32.85369975852083}\n",
      "iteration:15 loss:{'ner': 43.62451931415551}\n",
      "iteration:16 loss:{'ner': 34.59295708849365}\n",
      "iteration:17 loss:{'ner': 27.449246805686233}\n",
      "iteration:18 loss:{'ner': 26.862203350958946}\n",
      "iteration:19 loss:{'ner': 29.085170731401615}\n",
      "iteration:20 loss:{'ner': 32.77868313574521}\n",
      "iteration:21 loss:{'ner': 18.32848528457404}\n",
      "iteration:22 loss:{'ner': 20.798471285129168}\n",
      "iteration:23 loss:{'ner': 37.67375401205572}\n",
      "iteration:24 loss:{'ner': 24.413500034646777}\n",
      "iteration:25 loss:{'ner': 19.005693369992997}\n",
      "iteration:26 loss:{'ner': 10.960965434784004}\n",
      "iteration:27 loss:{'ner': 20.630740804773776}\n",
      "iteration:28 loss:{'ner': 21.315571828388638}\n",
      "iteration:29 loss:{'ner': 9.755403035829216}\n",
      "iteration:30 loss:{'ner': 14.095225635248974}\n",
      "iteration:31 loss:{'ner': 12.054672945824823}\n",
      "iteration:32 loss:{'ner': 20.4229717059185}\n",
      "iteration:33 loss:{'ner': 17.05442721482563}\n",
      "iteration:34 loss:{'ner': 15.205805234635031}\n",
      "iteration:35 loss:{'ner': 4.126992695037027}\n",
      "iteration:36 loss:{'ner': 6.624398832156249}\n",
      "iteration:37 loss:{'ner': 5.016910039051748}\n",
      "iteration:38 loss:{'ner': 9.257601768558443}\n",
      "iteration:39 loss:{'ner': 11.443308710382874}\n",
      "iteration:40 loss:{'ner': 6.709379805403972}\n",
      "iteration:41 loss:{'ner': 6.922307318922597}\n",
      "iteration:42 loss:{'ner': 2.544283040098627}\n",
      "iteration:43 loss:{'ner': 6.6438570340397085}\n",
      "iteration:44 loss:{'ner': 6.624441193182191}\n",
      "iteration:45 loss:{'ner': 12.933366220220716}\n",
      "iteration:46 loss:{'ner': 4.047658626023181}\n",
      "iteration:47 loss:{'ner': 10.247195794388594}\n",
      "iteration:48 loss:{'ner': 11.564167177090093}\n",
      "iteration:49 loss:{'ner': 7.599950753871395}\n",
      "iteration:50 loss:{'ner': 3.7889182875105707}\n"
     ]
    }
   ],
   "source": [
    "optimized_dropout_rate=0.5\n",
    "n_iter=50\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            #print(annotations)\n",
    "            nlp.update(\n",
    "                [text],  # batch of texts\n",
    "                [annotations],  # batch of annotations\n",
    "                drop=optimized_dropout_rate,  # dropout - make it harder to memorise data\n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "        print(\"iteration:\"+str(itn+1)+\" loss:\"+str(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to C:\\Users\\nitikadam\\Desktop\\Python_Notebooks\\Resume_Cleaning_And_Classification\\Model\\saved_model_1\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"C:\\\\Users\\\\nitikadam\\\\Desktop\\\\Python_Notebooks\\\\Resume_Cleaning_And_Classification\\\\Model\\\\saved_model_1\\\\\" \n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from C:\\Users\\nitikadam\\Desktop\\Python_Notebooks\\Resume_Cleaning_And_Classification\\Model\\saved_model_1\n",
      "himanshu kumar singh sr. systems engineer infosys ltd chandigarh +91-8249164460 himanshusinghcs0036@gmail.com date of birth: 5th jan 1993 designation senior systems engineer key role production support data analyst hadoop developer etl developer project 1: prime therapeutics overview: develop migrate etl pipeline on big data which includes putting the data from fs to hive and apply data quality and data validation rule and create report on daily weekly and monthly basis.\n",
      "[('91-8249164460', 'demographic_information_contact'), ('himanshusinghcs0036@gmail.com', 'demographic_information_email'), ('hadoop developer', 'role_information')]\n",
      "responsibilities: writing spark scala code for data processing.\n",
      "[]\n",
      "skills learned: spark aws emr s3 aurora db athena redshift mysql python hive unix airflow scheduler.\n",
      "[]\n",
      "used hadoop stack hive spark airflow scheduler unix.\n",
      "[]\n",
      "writing python code for database connection and write java rest service to connect ui and backend service.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "predicted_annotations=[]\n",
    "predicted_entities=[]\n",
    "for text, _ in TEST_DATA:x\n",
    "    print(text)\n",
    "    doc = nlp2(text)\n",
    "    #print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    #print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    result= [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    print(result)\n",
    "    predicted_annotations.append([ent.text for ent in doc.ents])\n",
    "    predicted_entities.append([ent.label_ for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['predicted_annotations'] = pd.Series(predicted_annotations, index = df_test.index[:len(predicted_annotations)])\n",
    "df_test['predicted_entities'] = pd.Series(predicted_entities, index = df_test.index[:len(predicted_entities)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['predicted_annotations', 'predicted_entities']\n",
    "for col in cols:\n",
    "    df_test[col] = df_test[col].map(lambda x: str(x).lstrip(\"[\").rstrip(\"[\")).astype(str)\n",
    "for col in cols:\n",
    "    df_test[col] = df_test[col].map(lambda x: str(x).lstrip(\"]\").rstrip(\"]\")).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities_actual=df_test[\"Entities\"].values\n",
    "annotations_actual=df_test[\"Annotation\"].values\n",
    "annotations_predicted=df_test['predicted_annotations'].values\n",
    "entities_predicted=df_test['predicted_entities'].values\n",
    "#labels=df_test[\"Entities\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demographic_information_contact' 'demographic_information_email'\n",
      " 'organization_information' 'demographic_information_address'\n",
      " 'role_information' 'technologies']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "csvfile=open('Confusion_Matrix_NER_NEW1.csv', 'w', newline='')\n",
    "spamwriter = csv.writer(csvfile)\n",
    "spamwriter.writerow(['Entity_Name','True Positives','False Positives','True Negatives','False Negatives', 'Precision', 'Recall','F-Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity_Name:demographic_information_contact\n",
      "True_Positives:1 False_Positives:0 False_Negatives:0 True_Negatives:5\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "Fscore: 1.0\n",
      "\n",
      "Entity_Name:demographic_information_email\n",
      "True_Positives:1 False_Positives:0 False_Negatives:0 True_Negatives:5\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "Fscore: 1.0\n",
      "\n",
      "Entity_Name:organization_information\n",
      "True_Positives:0 False_Positives:0 False_Negatives:1 True_Negatives:4\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "Fscore: 0\n",
      "\n",
      "Entity_Name:demographic_information_address\n",
      "True_Positives:0 False_Positives:0 False_Negatives:1 True_Negatives:4\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "Fscore: 0\n",
      "\n",
      "Entity_Name:role_information\n",
      "True_Positives:0 False_Positives:0 False_Negatives:1 True_Negatives:5\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "Fscore: 0\n",
      "\n",
      "Entity_Name:technologies\n",
      "True_Positives:0 False_Positives:0 False_Negatives:4 True_Negatives:1\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "Fscore: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(labels)):\n",
    "    true_positives=0\n",
    "    false_positives=0\n",
    "    false_negetives=0\n",
    "    true_negetives=0\n",
    "    for i in range(len(entities_actual)):\n",
    "        pred_ent=str(entities_predicted[i]).split(\",\")\n",
    "        act_ent=str(entities_actual[i]).split(\",\")\n",
    "        pred_annot=str(annotations_predicted[i]).split(\",\")\n",
    "        act_annot=str(annotations_actual[i]).split(\",\")\n",
    "        for j in range(len(act_ent)):\n",
    "            for l in range(len(pred_ent)):\n",
    "                if (labels[k] in act_ent[j]):\n",
    "                    if(labels[k] in pred_ent[l]):\n",
    "                        if act_annot[j] in pred_annot[l]:\n",
    "                            true_positives=true_positives+1\n",
    "                        else:\n",
    "                            false_negetives=false_negetives+1\n",
    "        \n",
    "        if (labels[k] in entities_predicted[i])&(labels[k] not in entities_actual[i]):\n",
    "            false_positives=false_positives+1\n",
    "        elif (labels[k] not in entities_predicted[i])&(labels[k] in entities_actual[i]):\n",
    "            false_negetives=false_negetives+1\n",
    "        else:\n",
    "            true_negetives=true_negetives+1\n",
    "            \n",
    "    if true_positives==0:\n",
    "        Precision=0\n",
    "        Recall=0\n",
    "        Fscore=0\n",
    "    else:\n",
    "        Precision=(true_positives)/(true_positives+false_positives)\n",
    "        Recall=(true_positives)/(true_positives+false_negetives)\n",
    "        Fscore=2*((Precision*Recall)/(Precision+Recall))\n",
    "    \n",
    "    print(\"Entity_Name:\"+str(labels[k]))    \n",
    "    print(\"True_Positives:\"+str(true_positives)+\" False_Positives:\"+str(false_positives)+\" False_Negatives:\"+str(false_negetives)+\" True_Negatives:\"+str(true_negetives))\n",
    "    print(\"Precision:\",round(Precision,2))\n",
    "    spamwriter.writerow([labels[k], true_positives,false_positives,true_negetives,false_negetives, round(Precision,2), round(Recall,2), round(Fscore,2)])\n",
    "    print(\"Recall:\",round(Recall,2))\n",
    "    print(\"Fscore:\",round(Fscore,2))\n",
    "    print(\"\")\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
